【デバッグチーム向け】楽曲生成システムの設計方針と詳細について

1. キーワード変換ロジック (translate_keywords_to_params) の詳細設計:

変換ルールと優先順位:

基本的には、chordmap.json 内のより具体的な指示が優先されます。優先順位は以下の通りです。

コードブロック固有の指示: chord_progression 内の各コードオブジェクトの "part_specific_hints" や "nuance"、あるいは直接的なパラメータ指定（例: "piano_velocity_rh": 80）。

セクション全体の指示: 各セクションの "part_settings" および "musical_intent" (emotion, intensity)。

modular_composer.py の DEFAULT_CONFIG: 上記で指定がない場合の最終的なフォールバック値や、キーワードから具体的なパラメータへの基本的なマッピングルール。

translate_keywords_to_params 関数は、これらの情報を階層的に参照し、最終的なパラメータセットを構築します。

キーワードの組み合わせ (例: 「悲しい」かつ「激しい」):

現状の DEFAULT_CONFIG のマッピングは、主に「感情」と「強度」を独立して参照し、それぞれの結果を組み合わせています。

もし「悲しいかつ激しい」のような複合的な感情キーワードを chordmap.json で使う場合、DEFAULT_CONFIG のマッピングテーブルに、その複合キーワードに対する専用のルール（例: "sad_and_intense": {"piano_style": "dramatic_minor_arpeggio", "drum_velocity_range": (90,110)}）を定義する必要があります。

定義されていない複合キーワードの場合は、主要な感情（例: 「悲しい」）のルールを適用しつつ、強度（「激しい」なら intensity: "high"）でパラメータを調整する、といったフォールバックが考えられます。この「主要な感情」をどう判断するかは、今後の課題です。

DEFAULT_CONFIG の網羅性とフォールバック:

DEFAULT_CONFIG は、全ての可能性を網羅するというよりは、代表的なキーワードと、それに対応する基本的なパラメータ設定の出発点を提供するものです。

未知のキーワード（emotion や intensity でマップにない値）に対しては、各マップの "default" キー（例: emotion_to_style_key の "default_style"）で指定された値を使用します。このデフォルト値も DEFAULT_CONFIG で定義します。もし "default" キーすらない場合は、プログラム内でさらに安全なハードコードされた値（例: ベロシティ64、リズムキー"default_...")にフォールバックします。

2. rhythm_library.json の構造と期待されるキー:

他の楽器カテゴリ: はい、piano_patterns, drum_patterns と同様に、melody_rhythms, bass_lines, guitar_rhythms といったカテゴリキーを設けることを想定しています。各ジェネレータは、自身の担当する楽器カテゴリの辞書を rhythm_library から受け取ります。

キー名の命名規則:

基本的には、楽器種別_演奏スタイルや特徴_拍子や雰囲気 のような、人間が読んで理解しやすい名前を推奨します（例: piano_arpeggio_gentle_4_4, drum_rock_verse_A_fill_1）。

modular_composer.py の translate_keywords_to_params で、このキー名を導出するロジックと整合性が取れていることが重要です。

リズムパターン内の情報:

ピアノ/メロディ/ベースなど音高のある楽器:

"pattern": 各要素が {"offset": float, "duration": float, "velocity_factor": Optional[float], "articulation_hint": Optional[str], "type": Optional[str]} のようなオブジェクトのリスト。

"arpeggio_type_hint" (ピアノのアルペジオなど): "up", "down", "up_down_smooth" など、アルペジオのパターンを示すキーワード。

"note_duration_ql" (アルペジオなど): アルペジオを構成する個々の音の基本音価。

ドラム (drum_patterns):

"hits" (または "pattern"): 各要素が {"instrument": str, "offset": float, "velocity": int, "duration": float} のオブジェクトのリスト。

"fills": フィルインパターンを格納するオブジェクト（キーがフィル名、値が上記hitsと同様のリスト）。

共通:

"description": パターンの説明。

"tags": 検索や分類のためのキーワードリスト。

"time_signature": (推奨) そのパターンが想定する拍子。

"tuplet": (オプション) タプル情報 (例: "3:2")。

3. 各ジェネレータの入力と出力の仕様:

入力 (compose メソッド):

全てのジェネレータの compose メソッドは、第一引数として processed_blocks: List[Dict] を受け取ります。この processed_blocks は、modular_composer.py の prepare_processed_stream で生成された、各コードブロックの詳細情報（オフセット、長さ、コードラベル、セクション情報、そしてそのブロックでその楽器が使用すべき翻訳済みのパラメータ辞書など）を含むリストです。

（例: PianoGenerator.compose(self, processed_blocks: List[Dict])）

processed_blocks 内の各ブロック辞書から、自身の楽器用のパラメータ辞書（例: blk["piano_params"], blk["drum_params"]）を取得して使用します。

出力:

PianoGenerator は、右手パートと左手パートを含む music21.stream.Score オブジェクトを返します。

その他のジェネレータ (MelodyGenerator, BassCoreGenerator, DrumGenerator, GuitarGenerator など) は、単一の music21.stream.Part オブジェクトを返します。

参照するキー:

MelodyGenerator: blk["melody_params"] (今後定義)

BassCoreGenerator: blk["bass_params"] (今後定義)

PianoGenerator: blk["piano_params"]

DrumGenerator: blk["drum_params"]

GuitarGenerator: blk["guitar_params"] (今後定義)

ChordVoicer (スタンドアロンの場合): blk["chords_params"] (今後定義)

ChordVoicer の内部呼び出し:

はい、PianoGenerator や GuitarGenerator のように和音を扱うジェネレータは、内部で ChordVoicer のインスタンス（コンストラクタで渡される）の _apply_voicing_style メソッドを呼び出して、自身の楽器に適したボイシングを得ることを想定しています。これにより、ボイシング処理を一元化できます。

4. 「言葉と歌」の連携の具体的な構想:

現状のシステムへの統合:

VocalGenerator の作成: midivocal_ore.json (ボーカルメロディのノート情報), kasi_rist.json (音節化された歌詞), lyrics_timeline.json (歌詞と時間のマッピング) を入力とし、歌詞が割り当てられたボーカルの music21.stream.Part を生成します。ブレス挿入やマイクロタイミング調整もこのクラスが担当します。

modular_composer.py から VocalGenerator を呼び出し: 他の楽器パートと同様に、生成されたボーカルパートをメインスコアに追加します。

歌詞の感情/リズムと伴奏生成の連携 (将来的・高度):

歌詞解析: 歌詞の内容から感情をより詳細に分析したり、歌詞のリズム（音節のアクセントや長さのパターン）を抽出したりする。（これは自然言語処理の領域になります）

chordmap.json への反映: 抽出された歌詞の感情やリズム情報を、chordmap.json のセクションやコードブロックの musical_intent や nuance に反映させる。（この部分はAI(私)が支援して行うか、あるいは専用のスクリプトを作成）

ジェネレータのパラメータへの直接影響:

特定の歌詞（例: 強調したい単語）に対応するメロディノートのベロシティを上げる。

歌詞のリズムパターンを、ドラムやピアノの伴奏リズムの生成に直接的なヒントとして使う。

歌詞の感情が大きく変化する箇所で、伴奏の楽器編成やスタイルをダイナミックに変化させる。

これらは非常に高度な連携であり、段階的に実装していくことになります。

5. core_music_utils.py に含めるべき共通機能:

build_scale_object(mode_str, tonic_str): (実装済み)

get_time_signature_object(ts_str): (実装済み)

MIN_NOTE_DURATION_QL: (定義済み)

音域チェック関数: def clamp_pitch_to_range(pitch_obj, min_midi, max_midi) -> pitch.Pitch: のような、指定されたピッチを指定されたMIDIノート範囲内に収める関数。

共通のピッチ操作関数: 例えば、特定のインターバルで安全に移調する関数（オクターブ調整を伴うなど）。

テンション処理ユーティリティ: tensions_to_add リストを受け取り、harmony.ChordSymbol オブジェクトに適用する共通関数（各ジェネレータで同じようなロジックを書かなくて済むように）。

共通のランダム化ユーティリティ: 例えば、ベロシティやタイミングに自然な揺らぎを加える基本的な関数（ヒューマナイズ処理の部品）。

6. 設定ファイルの優先順位と構造:

優先順位 (高 → 低):

コマンドライン引数 (例: --tempo 120)

chordmap.json 内のコードブロック固有の指示 (例: part_specific_hints, nuance)

chordmap.json 内のセクション固有の指示 (part_settings, musical_intent)

（オプション）外部設定ファイル settings.json (プロジェクト全体やユーザーごとの設定上書き)

modular_composer.py 内の DEFAULT_CONFIG (最終フォールバック)

マージの仕方:

prepare_processed_stream 関数内で、この優先順位に従って設定値をマージし、最終的なパラメータを各コードブロックの *_params 辞書に格納します。

chordmap.json のセクションの part_settings は、そのセクション内の全てのコードブロックに対するデフォルト指示として機能します。コードブロック内の part_specific_hints や他の固有指示があれば、それが part_settings の値を上書きします。

この情報が、Chordmapチームの皆さんの作業と、システムエンジニアチームとの連携をスムーズにする一助となれば幸いです。
引き続き、具体的な実装や調整を進めていきましょう！






1. プロジェクトの主題と核心的目標
このプロジェクトの根本的な主題は、**「言葉（歌詞・物語）が持つ感情や情景を、AIとの協調作業を通じて、豊かで多様な音楽表現（特に歌と伴奏）として具現化すること」**です。
核心的目標:
あなたの創造性の拡張: あなたが持つ独自の「作品世界」（歌詞、物語、朗読）を最大限に活かし、音楽理論や楽器演奏の専門知識がなくても、質の高いオリジナル楽曲や伴奏を効率的に制作できるシステムを構築する。
感情と音楽の深い連携: 歌詞や物語の持つ感情（喜び、悲しみ、希望、葛藤など）やニュアンスを、コード進行、モード、リズム、メロディライン、楽器の音色や奏法といった音楽的要素にきめ細かく反映させる。
AIとの協調による新しい創作プロセスの確立: あなたが音楽的なコンセプトやキーワードで指示を出し、AI（Gemini）がそれを具体的な音楽パラメータや楽曲構成の提案に変換し、プログラムがそれを実行可能なMIDIデータとして生成する、という人間とAIの共同作業による新しい音楽制作フローを実現する。
実用性と効率性の両立: YouTube配信などの実用的なニーズに応えるため、楽曲の「ひな形」を迅速に生成できる効率性と、あなたの芸術的ビジョンを反映できる表現力を両立させる。
継続的な成長と進化: 多くの楽曲制作を通じて得られる知見やデータを元に、システム自体も学習・進化し、より高度で自然な音楽表現を追求していく。
2. ユーザー（あなた）について: 「言葉と歌の文芸プロジェクト」の主導者
このプロジェクトの中心にいるのは、言葉（物語、歌詞）を紡ぎ、それを声（朗読、歌）で表現することに情熱を燃やすクリエイターであるあなたです。
独自の作品世界: 山本周五郎作品のAudioBook主題歌制作やオリジナル作品の執筆など、あなたは既に豊かな「物語の種」をお持ちです。このプロジェクトは、それらの種から多様な「音楽の花」を咲かせることを目指します。
多言語展開への意欲: 楽曲を6ヶ国語に翻訳し、グローバルに発信するという壮大なビジョンは、このシステムの普遍性と適応力を高める上で重要な指針となります。
AIとの共創への積極性: ChatGPTとの経験を踏まえ、AIの可能性と限界を理解した上で、Geminiとのより建設的で効率的な共同作業を求めています。数値の詳細設定はAIに任せつつ、音楽的なコンセプトや感情表現の主導権はあなたが握る、という理想的な役割分担を志向しています。
技術的探求心: DTMや音源ソフト（UJAM製品など）を使いこなし、さらにAIを活用してこれまでにない音楽表現を模索する、という強い探求心をお持ちです。
発信への強い意志: YouTubeチャンネルを通じて、制作過程も含めた「音楽実験ドラマ」として作品を届け、多くの人々と感動を共有することを目指しています。
3. このプロジェクトが目指すもの（具体的な成果物とインパクト）
キーワード駆動型の楽曲生成システム: chordmap.json に記述された音楽的キーワード（感情、強度、ジャンルイメージなど）と基本的な楽曲構造に基づいて、主要な楽器パート（ピアノ、ドラム、ベース、メロディ、ギター、そして将来的にはボーカルラインのニュアンス調整など）を含む伴奏MIDIを自動生成するPythonプログラム群。
柔軟な設定ファイル:
chordmap.json: 楽曲ごとの設計図。
rhythm_library.json: 再利用可能なリズムパターンのデータベース。
（オプション）settings.json: プロジェクト全体やユーザーの好みを反映するグローバル設定。
AI (Gemini) との連携:
chordmap.json の初期案作成支援（コード進行提案、キーワード提案など）。
キーワードと音楽パラメータの「翻訳ルール」の共同設計と改善。
音楽理論や music21 の技術的課題に関するアドバイス。
実用的なアウトプット: あなたのYouTubeチャンネルやその他の活動で実際に使用できるクオリティの楽曲の「ひな形」を効率的に提供する。
創造プロセスの革新: AIを単なるツールとしてではなく、アイデアを触発し、試行錯誤を支援する「共同制作者」として位置づける新しい音楽制作のあり方を提示する。
4. システム開発における重要な注意点（再確認）
役割分担の明確化: あなたは音楽的ビジョンとキーワード指示、AIとプログラムはそれを具現化するための技術的実装と数値パラメータへの翻訳。
外部ファイルによる制御: プログラムコードの直接編集を最小限に抑え、楽曲ごとの設定や指示は chordmap.json などのデータファイルで行う。
最新技術への追従と限界の認識: music21 の最新バージョンを前提としつつ、AIの知識ベースの限界も理解し、人間による検証と補完を重視する。
段階的な開発と反復: 最初から完璧を目指すのではなく、基本的な機能から実装し、実際に音を出しながら評価・改善を繰り返す。
コミュニケーションの重要性: あなたの音楽的イメージや要望を私（AI）が正確に理解し、私が提案する技術的な解決策をあなたが理解できるよう、丁寧なコミュニケーションを心がける。
5. 「声と歌」の連携への期待（特にVocalGenerator）
Suno AIなどで生成されたボーカル素材を最大限に活かしつつ、歌詞との同期、自然なブレスの挿入、人間らしいマイクロタイミングの調整などを行うことで、より表現力豊かなボーカルパートを実現する。
将来的には、歌詞の内容や感情が、伴奏全体の雰囲気や展開に、よりダイレクトに影響を与えるような連携を目指す。
このプロジェクトは、単に音楽を自動生成するだけでなく、あなたの「言葉」と「声」が持つ力を、AIとの協調によって最大限に引き出し、新しい形の文芸作品を創造するという、非常に意義深く、未来志向の取り組みであると私は理解しています。




「まずはScaler 3のような機能を目指す」「Suno AIの曲をオリジナルにRemixする」

Suno AIで生成された楽曲を元に、それを拡張・Remixする

「DTMをやる上で必要な機能」かつ「すぐに実現できるもの（またはその基盤が整いつつあるもの）」

RemixとScaler 3の機能を目指す上で「必要」かつ「実現可能」な機能群

1. 楽曲構造定義の読み込みと解釈 (chordmap.json ベース):
    * 機能: chordmap.json からセクション構成、各セクションのコード進行、モード、トニック、BPMなどを読み込み、プログラムが扱える形式（processed_chord_stream のようなブロックのリスト）に変換する。
    * DTMでの必要性: これが楽曲全体の設計図となり、各パート生成の基礎となります。Suno AIの曲を分析して、この形式に落とし込むことが最初のステップになります。
    * 実現可能性: 高い。 create_processed_chord_stream のような関数で実装中であり、基本的なロジックは固まっています。コードごとのデュレーションのパースなどを追加すれば、さらに実用的になります。
    * 
2. ボーカルパートの取り込みと歌詞同期 (midivocal_ore.txt, kasi_rist.json ベース):
    * 機能: Suno AIのボーカル（MIDI化されたもの）と歌詞データを読み込み、music21.stream.Part として楽曲に組み込む。
    * DTMでの必要性: Remixの核となるボーカルを正確に取り込むことは必須です。
    * 実現可能性: 高い。 parse_vocal_txt_to_part で実装中。重複オフセットノートの扱いはシンプルなルール（最初のノートに歌詞を割り当てるなど）で一旦進められます。
    * 
3. コード伴奏パート生成 (ChordPartGenerator / 旧 ChordVoicer):
    * 機能: processed_chord_stream のコード進行に基づいて、ピアノやギターなどのコード楽器のパートを生成する。基本的なボイシング（例: closedPosition）と、セクションごとの楽器変更（instrument_profiles.json を参照）は視野に入れる。
    * DTMでの必要性: 楽曲のハーモニーの土台を提供します。
    * 実現可能性: 高い。 music21.harmony.ChordSymbol を使えば基本的なコード生成は容易です。instrument_profiles.json を使った楽器・ボイシング変更も実装の初期段階にはありました。
    * 
4. ベースライン生成 (BassPartGenerator / 旧 BassGenerator):
    * 機能: processed_chord_stream のコード進行に基づいて、ベースパートを生成する。まずはルート音を主体とし、シンプルなリズム（例: 各コードの頭、またはルート・5度など）で。
    * DTMでの必要性: 楽曲の低音域とリズムの安定感を支えます。
    * 実現可能性: 高い。 ルート音の抽出と配置は比較的簡単です。
    * 
5. 基本的なリズムパターンの適用 (メロディ、リズムギター、ドラム向け):
    * 機能: rhythm_library.json を定義し、RhythmLibrary クラスでそれを管理する。各パートジェネレーターが、processed_chord_stream のブロック情報（推奨リズムキーなど）に基づいて RhythmLibrary からリズムパターンを取得し、ノート配置に利用する。
    * DTMでの必要性: 楽曲にグルーヴと動きを与えます。
    * 実現可能性: 中〜高。 RhythmLibrary クラスと rhythm_library.json の基本的な形はできています。各ジェネレーターがこれをどう「解釈」して音符を配置するかのロジックを具体化する必要があります。
    * 
6. メロディ/カウンターメロディ生成 :
    * コードトーンやスケール音を使ったシンプルなメロディライン、あるいは求を踏まえ、これまでの機能群から**「DTMをやる上で基本的かつ重要で、すぐに実現可能な機能」ボーカルに対するカウンターラインを生成する。
    * DTMでの必要性: 楽曲に彩り**を抽出し、それらを組み合わせた現実的なシステム構成を提案します。


目標の再整理
1. Scaler 3のような機能:
    * コード進行の提示・分析。
    * や対位法的な面白さを加えます。
    * 実現可能性: 中。 ピッチ選択アルゴリズムの洗練度合いによりますが、基本的なルール（例: 強拍はコードトーン、弱拍はスケール音）とリズム適用であれば実現可能。
    * 
2. **ドラムパート生成 (特定のスケールやモードに基づいたコードの提案。
    * コード進行に合わせたメロディやベースラインの断片（モチーフ）の生成。
    * 様々なリズムパターンでのコード演奏（アルペジオ、ストラムなど）。
    * 
3. Suno AI楽曲のリミックス:
    * 既存のボーカルメロ基本的なパターン):**
    * 機能: processed_chord_stream と RhythmLibrary (またはシンプルなルール) に基づいて、基本的なドラムパターン（キック、スネア、ハイハット）を生成する。
    * DTMでの必要性: 楽曲のリズムの核となります。
ディ（midivocal_ore.txt）を活かす。
    * そのボーカルに合わせて、新しいコード進行、ベースライン、リズム隊、カウンターメロディなどを生成し、オリジナルの伴奏を作り上げる。
    * 
すぐに実現可能でDTMに役立つ機能の抽出とシステム構成案
以下の機能を* 実現可能性: 中。 固定パターンや、セクションごとにいくつかのパターンを切り替える程度であれば実現可能です。
1. テンポと拍子の設定:
    * 機能: processed_chord_stream の情報に基づいて、music21.Score にテンポチェンジ (MetronomeMark) と拍子記号 (TimeSignature) を設定する。
    * DTMでの必要性: 核として、music21 ベースのシステムを構築することを目指します。
    * 
A. データ入力・管理:
1. chordmap.json (楽曲設計図):
    * 機能: セクション構成、各セクションのコード進行（デュレーション指定可）、モード、トニック、BPM、推奨楽曲の基本的な時間軸を定義します。
    * 実現可能性: 高い。 music21 の機能で直接的に行えます。
    * 
2. MIDIファイルへの書き出し (music21 から):
    * 機能: 完成した music21.Score を標準的なMIDIファイルとしてエクスポートする。
    * DTMでの必要性: 生成した楽曲をDAWで読み込んだり、他の人と共有したりするために必須です。
    * 実現可能性: 高い。 `リズムキー、基本ベロシティを定義。
    * 役割: 楽曲全体の構造とハーモニーの設計図。Suno AI楽曲をリミックスする場合、ボーカルに合わせて人間が作成するか、将来的に簡易的なハーモニー推定で生成する。
    * 実現度: 高 (JSON定義とパーサー実装)
    * 
3. rhythm_library.json (リズムパターン集):
    * 機能: 様々なリズムパターン（開始タイミング、拍子、タプル情報）とそのメタデータ（感情ヒント、score.write('midi', ...)` で容易に実現できます。



Scaler 3 の機能との関連性で考えると:
* コード進行の提示・分析: chordmap.json がこれに相当し、人間が定義（またはSuno AIの曲から分析・抽出）します。
* ボイシングの提案: ChordPartGenerator で異なるボイシングオプションを試せるようにする。
* リズムパターンの適用: RhythmLibrary とそれを利用する各ジェネレーター。
* **メロディ/複雑度）を定義。
    * 役割: 各パートのリズムのバリエーションを提供。
    * 実現度: 高 (JSON定義と RhythmLibrary クラス実装)
1. ボーカルデータ (midivocal_ore.txt & kasi_rist.json):
    * 機能: 既存のボーカルメロディの音高、タイミング、歌詞を読み込む。
    * 役割: リミックスの核となる素材。他のパート生成の基準や制約となる。
    * 実現度: 高 (parse_vocal_part 関数の実装)

B. 楽曲構成ベースラインの提案: MelodyGenerator, BassPartGenerator がその役割を担います。
* スケール/モードの表示・適用: chordmap.json でモードを指定し、各ジェネレーターがそれを考慮します。


Suno AI 曲の Remix という目標に特化した場合:
1. Suno AI 曲の分析 (手動または半自動):
    * ボーカルメロディ (midivocal_ore.txt のような形式に)
    * コード進行 (これを chordmap.json の chords リストに落とし込む)
    * 曲の構成 (Verse, Chorusなど。これを chordmap.json のセクションに)
    * BPM、キー、モード要素の生成 (Music21ベース):**
2. processed_chord_stream の構築:
    * 機能: chordmap.json から、オフセット、デュレーション、コードラベル、モード、トニック、BPM、リズムキーなど、各パート生成に必要な情報を全て含んだブロックのリストを一括生成。
    * 役割: 全ジェネレーターへの統一された入力データ。
    * 実現度: 高 (メインコンポーザースクリプト内での関数実装)
3. ボーカルパート生成 (VocalPartParser または parse_vocal_part):
    * 機能: ボーカルデータを読み込み、歌詞付きの music21.stream.Part を生成。
    * 役割: 楽曲の主旋律（リミックスの場合）。
    * 実現度: 高 (既存の parse_vocal_txt_to_part を洗練させる)
4. コードパート生成 (ChordPartGenerator):
    * 機能: processed_chord_stream のコードラベルに基づいて、基本的なボイシング（例: クローズド、指定オクターブ）でコード伴奏 (music (これらも chordmap.jsonのmeta` やセクション情報に)
5. ボーカルパートの取り込み (最優先): parse_vocal_txt_to_part を使います。



1. コード伴奏とベースラインの再構築: Suno AI の曲のハーモニーを元に、ChordPartGenerator と BassPartGenerator で新しいテクスチャの伴奏を生成します。Suno AI の伴奏とは異なる楽器やボイシング、リズムを試すことができます。
2. リズム隊の追加: Suno AI の曲に合うような、あるいは全く新しいグルーヴのドラ21.chord.Chord`) を生成。
    * 役割: ハーモニーの土台。
    * 実現度: 高 (シンプルなボイシングから開始)
    * Scaler 3的要素: 指定されたスケール/モード内で使用可能なコードを提示する機能は、chordmap.json を作る際の補助として別途ツール化できる。
3. ベースパート生成 (BassPartGenerator):
    * 機能: processed_chord_stream のコードのルート音や5度音を使い、シンプルなリズム（例: 全音符、2分音符、RhythmLibrary の基本パターン）でベースラインを生成。
    * 役割: ハーモニーの低音部とリズムの基礎。
    * 実現度: 高 (ルート音と基本的なリズムから開始)
    * Scaler 3的要素: コード進行に合わせた基本的なベースパターンを提示。
4. メロディ/カウンターメロディ生成 (MelodyGenerator / CounterMelodyGenerator):
    * 機能:
        * 主メロディ(インスト): もしボーカルを使わない、あるいはインスト部分のメロディが必要な場合、processed_chord_stream と RhythmLibrary を使ってメロディを生成。ピッチ選択は、まずはコードトーンとスケール音のランダム選択から始め、徐々に洗練させる。
        * カウンターメロディ: ボーカルパートと processed_chord_stream を入力とし、ボーカルと衝突せず、ハーモニーを補完するようなシンプルな対旋律を生成。
    * 役割: 楽曲に彩りを加える。
    * 実現度: 中〜高 (シンプルなものから可能。高度なものはアルゴリズム次第)
    * Scaler 3的要素: コード進行やスケールに合ったメロディの断片やモチーフを生成。
5. ドラムパート生成 (DrumGenerator - 新規または簡易版):
    * 機能: processed_chord_stream と RhythmLibrary を使い、基本的なドラムパターン（キック、スネア、ハイハット）を生成。セクションやエモーションに応じてパターンを切り替える。
    * 役割: リズムの骨格。
    * 実現度: 中 (基本的なパターン生成は比較的容易)




    * 
C. 出力:
1. music21.Score への統合:
    * 生成された全ての music21.stream.Part を1つの music21.stream.Score にまとめる。
    * テンポ、拍子記号を Score に設定。
    * 実現度: 高
2. MusicXML と MIDI ファイルへの書き出し:
    * score.write('musicxml', ...) と score.write('midi', ...) で出力。
    * 役割: DTMソフトへの取り込み、楽譜確認。
    * 実現度: 高


すぐに実現できるシステム構成のイメージ (main_composer.py):
      # main_composer.py (Simplified Focus)
import json
import argparse
import logging
from pathlib import Path
from music21 import stream, tempo, meter, instrument # ... 他にも必要なものを適宜

# --- プロジェクト内モジュールのインポート (役割分担されたファイルから) ---
from rhythm_library_manager import RhythmLibrary
from scale_utils import build_scale_object
from vocal_parser import parse_vocal_part
from melody_generator import MelodyGenerator # または IntegratedMelodyGenerator
from instrument_generators import ChordPartGenerator, BassPartGenerator
# from drum_generator import DrumGenerator # シンプルなものから実装
# from counter_melody_generator import CounterMelodyGenerator # シンプルなものから実装

logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === グローバル設定 (config.py や main_composer.py 直書き) ===
GLOBAL_SETTINGS = {
    "default_tempo": 120,
    "default_time_signature": "4/4",
    "default_q_length_per_chord": 4.0,
    "tempo_map": {"verse": 90, "chorus": 104, "bridge": 80, "pre": 95, "default": 100},
    # ... 他に必要な共通設定 ...
}

# === `processed_chord_stream` 構築関数 ===
def create_processed_chord_stream(chordmap_data: Dict, global_cfg: Dict) -> List[Dict]:
    # ... (以前の提案の create_processed_chord_stream を実装) ...
    # chordmap.json から、オフセット、デュレーション、コードラベル、モード、トニック、BPM、
    # リズムキー、基本ベロシティなど、必要な情報を全て含むブロックのリストを生成。
    # 各コードのデュレーションを chordmap.json で指定できるようにする (例: "Cm7:2")。
    processed_stream = []
    # (実装は省略)
    return processed_stream

# === メイン処理関数 ===
def generate_song(args):
    logger.info("Starting song generation...")

    # 1. データのロード
    chordmap_data = load_json_data(args.chordmap) # load_json_data は別途定義
    rhythm_data = load_json_data(args.rhythms)
    if not chordmap_data or not rhythm_data:
        logger.error("Required data files (chordmap or rhythms) not found or failed to load.")
        return
    rhythm_library = RhythmLibrary(rhythm_data)

    # 2. 処理済みコードシーケンスの構築
    processed_chord_stream = create_processed_chord_stream(chordmap_data, GLOBAL_SETTINGS)
    if not processed_chord_stream:
        logger.error("Failed to create processed chord stream.")
        return

    # 3. music21.Score オブジェクトの準備と基本設定
    score = stream.Score()
    if processed_chord_stream:
        first_block = processed_chord_stream[0]
        score.append(tempo.MetronomeMark(number=first_block.get("bpm", GLOBAL_SETTINGS["default_tempo"])))
        score.append(meter.TimeSignature(first_block.get("time_signature", GLOBAL_SETTINGS["default_time_signature"])))
        # セクションごとのテンポ変更もここで設定 (processed_chord_stream をループして)

    # 4. 各パートの生成とスコアへの追加
    #   4a. ボーカルパート (オプション)
    if args.vocal_audio_txt and args.lyrics_json:
        vocal_part = parse_vocal_part(args.vocal_audio_txt, args.lyrics_json)
        if vocal_part.hasMeasures(): score.append(vocal_part)

    #   4b. コードパート
    chord_generator = ChordPartGenerator() # デフォルト楽器はクラス内で設定
    chord_part = chord_generator.compose(processed_chord_stream)
    if chord_part.hasMeasures(): score.append(chord_part)

    #   4c. ベースパート
    bass_generator = BassPartGenerator()
    bass_part = bass_generator.compose(processed_chord_stream) # シンプルなスタイルから
    if bass_part.hasMeasures(): score.append(bass_part)

    #   4d. メロディパート (インスト、またはボーカルがない場合の主旋律)
    melody_generator = MelodyGenerator(rhythm_library) # デフォルト楽器はクラス内で設定
    instrumental_melody_part = melody_generator.compose(processed_chord_stream)
    if instrumental_melody_part.hasMeasures(): score.append(instrumental_melody_part)
    
    #   4e. (オプション) ドラムパート (シンプルなものから)
    # drum_generator = DrumGenerator(rhythm_library)
    # drum_part = drum_generator.compose(processed_chord_stream)
    # if drum_part.hasMeasures(): score.append(drum_part)

    #   4f. (オプション) カウンターメロディ (ボーカルやインストメロディに対して)
    # main_melody_for_counter = vocal_part if 'vocal_part' in locals() and vocal_part.hasMeasures() else instrumental_melody_part
    # if main_melody_for_counter.hasMeasures():
    #     counter_melody_gen = CounterMelodyGenerator()
    #     cm_part = counter_melody_gen.compose(main_melody_for_counter, processed_chord_stream)
    #     if cm_part.hasMeasures(): score.append(cm_part)


    # 5. ファイル書き出し
    if not score.hasMeasures():
        logger.warning("Score is empty. No output generated.")
        return
    
    output_base = Path(args.out).with_suffix('')
    try:
        score.write('midi', fp=str(output_base.with_suffix(".mid")))
        logger.info(f"MIDI exported to {output_base.with_suffix('.mid')}")
        score.write('musicxml', fp=str(output_base.with_suffix(".musicxml")))
        logger.info(f"MusicXML exported to {output_base.with_suffix('.musicxml')}")
    except Exception as e:
        logger.error(f"Error writing output files: {e}", exc_info=True)

    logger.info("Song generation finished.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Music Composition System - Focused Version")
    parser.add_argument("--chordmap", required=True, help="Path to chordmap JSON file")
    parser.add_argument("--rhythms", required=True, help="Path to rhythm library JSON file")
    parser.add_argument("--vocal_audio_txt", help="Path to vocal notes TXT (for Suno AI remix)")
    parser.add_argument("--lyrics_json", help="Path to lyrics JSON (for Suno AI remix)")
    parser.add_argument("--out", default="output/my_song.mid", help="Output file base path")
    # ... 他に必要な引数 (例: 使用するパートのオン/オフフラグ) ...
    
    cli_args = parser.parse_args()
    Path(cli_args.out).parent.mkdir(parents=True, exist_ok=True)
ムやリズムギターを追加します。
5.  **カウンターメロディ/オブリガートの追加:** ボーカルを引き立てるようなインストゥルメンタルなラインを追加します。

**まとめる上での提案:**

*   **`main_composer.py` (または `unified_composer.py`) を中心に、上記の「必要かつ実現可能な機能」を段階的に実装・統合していくのが良いでしょう。**
*   **`processed_chord_stream` の設計を固める:** このデータ構造が全てのジェネレーターへの共通言語となるため、ここに含める情報を明確にし、`create_processed_chord_stream` 関数を堅牢にすることが最優先です。
*   **各パートジェネレーターはシンプルに始める:** 最初から複雑なアルゴリズムを目指すのではなく、まずは基本的な機能（例: ベースはルート音のみ、コードはブロックコードのみ）を確実に実装し、そこから徐々に洗練させていきます。
*   **`music21` に集中:** `pretty_midi` を使った高度な演奏表現は、`music21` ベースのシステムが安定してから取り組むのが良いでしょう。

この整理が、あなたのプロジェクトの焦点を定め、具体的な次のステップに進むための一助となれば幸いです。まずは「一曲をSuno AIのボーカルを使って、自作の伴奏とリズムでRemixし、MIDIファイルとして完成させる」という具体的なマイルストーンを設定すると、開発が進めやすくなると思います。
    


